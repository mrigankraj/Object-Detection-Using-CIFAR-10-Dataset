{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lN5GsvpTApd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3963b47f-7233-4251-9fe5-602f346c9044"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size    = 128\n",
        "epochs        = 100\n",
        "iterations    = 391\n",
        "num_classes   = 10\n",
        "mean          = [125.307, 122.95, 113.865]\n",
        "std           = [62.9932, 62.0887, 66.7048]\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', input_shape=(32,32,3)))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal'))\n",
        "    model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal'))\n",
        "    model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal'))\n",
        "    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 100:\n",
        "        return 0.01\n",
        "    \n",
        "    return 0.001\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train.shape,x_test.shape,y_train.shape,y_test.shape    \n",
        "    \n",
        "    \n",
        "    for i in range(3):\n",
        "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
        "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
        "        \n",
        "    \n",
        "    model = build_model()\n",
        "    print(model.summary())\n",
        "    # set callback\n",
        "    tb_cb = TensorBoard(log_dir='./lenet_dp_da', histogram_freq=0)\n",
        "    change_lr = LearningRateScheduler(scheduler)\n",
        "    cbks = [change_lr,tb_cb]\n",
        "\n",
        "    \n",
        "    print('Using real-time data augmentation.')\n",
        "    datagen = ImageDataGenerator(horizontal_flip=True,\n",
        "            width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "     \n",
        "    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                        steps_per_epoch=iterations,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=cbks,\n",
        "                        validation_data=(x_test, y_test))\n",
        "    \n",
        "scores = model.evaluate(x_train, y_train, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "model.save('Filename.h5')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 16:52:41.843956 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 16:52:41.873425 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 16:52:41.878954 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0720 16:52:41.903330 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0720 16:52:41.969418 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0720 16:52:41.977704 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 6)         456       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 120)               48120     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0720 16:52:44.700045 140264421463936 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0720 16:52:44.762978 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0720 16:52:47.978569 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0720 16:52:47.979903 140264421463936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 20s 50ms/step - loss: 1.7632 - acc: 0.3424 - val_loss: 1.5297 - val_acc: 0.4272\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.5199 - acc: 0.4415 - val_loss: 1.4176 - val_acc: 0.4821\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.4329 - acc: 0.4791 - val_loss: 1.4159 - val_acc: 0.5021\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.3726 - acc: 0.5035 - val_loss: 1.3103 - val_acc: 0.5284\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.3356 - acc: 0.5184 - val_loss: 1.2135 - val_acc: 0.5641\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.3031 - acc: 0.5321 - val_loss: 1.2457 - val_acc: 0.5517\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.2772 - acc: 0.5430 - val_loss: 1.1887 - val_acc: 0.5743\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.2515 - acc: 0.5507 - val_loss: 1.1503 - val_acc: 0.5938\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.2326 - acc: 0.5587 - val_loss: 1.2179 - val_acc: 0.5750\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.2141 - acc: 0.5663 - val_loss: 1.1384 - val_acc: 0.6009\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1950 - acc: 0.5756 - val_loss: 1.1724 - val_acc: 0.5886\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1809 - acc: 0.5821 - val_loss: 1.1119 - val_acc: 0.6179\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1689 - acc: 0.5839 - val_loss: 1.2138 - val_acc: 0.5854\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1625 - acc: 0.5851 - val_loss: 1.0885 - val_acc: 0.6235\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1420 - acc: 0.5936 - val_loss: 1.1879 - val_acc: 0.5960\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.1335 - acc: 0.5971 - val_loss: 1.0491 - val_acc: 0.6357\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.1233 - acc: 0.6018 - val_loss: 1.1208 - val_acc: 0.6209\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.1171 - acc: 0.6051 - val_loss: 1.0878 - val_acc: 0.6224\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.1036 - acc: 0.6115 - val_loss: 1.0571 - val_acc: 0.6311\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 1.0971 - acc: 0.6118 - val_loss: 1.0314 - val_acc: 0.6471\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0828 - acc: 0.6178 - val_loss: 1.0495 - val_acc: 0.6354\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0834 - acc: 0.6192 - val_loss: 1.0715 - val_acc: 0.6301\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0760 - acc: 0.6209 - val_loss: 1.0178 - val_acc: 0.6487\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0652 - acc: 0.6265 - val_loss: 1.0817 - val_acc: 0.6351\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0581 - acc: 0.6271 - val_loss: 1.0241 - val_acc: 0.6484\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0466 - acc: 0.6327 - val_loss: 1.0502 - val_acc: 0.6462\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0499 - acc: 0.6307 - val_loss: 0.9685 - val_acc: 0.6651\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0411 - acc: 0.6362 - val_loss: 1.0405 - val_acc: 0.6444\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.0323 - acc: 0.6389 - val_loss: 0.9535 - val_acc: 0.6699\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0250 - acc: 0.6400 - val_loss: 1.0033 - val_acc: 0.6543\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0264 - acc: 0.6407 - val_loss: 0.9430 - val_acc: 0.6705\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.0145 - acc: 0.6428 - val_loss: 0.9996 - val_acc: 0.6584\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 1.0165 - acc: 0.6452 - val_loss: 0.9991 - val_acc: 0.6537\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 1.0099 - acc: 0.6485 - val_loss: 1.0125 - val_acc: 0.6564\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0059 - acc: 0.6471 - val_loss: 0.9664 - val_acc: 0.6657\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 1.0053 - acc: 0.6482 - val_loss: 0.9291 - val_acc: 0.6777\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9884 - acc: 0.6532 - val_loss: 1.0391 - val_acc: 0.6556\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9920 - acc: 0.6524 - val_loss: 0.9832 - val_acc: 0.6609\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9864 - acc: 0.6563 - val_loss: 0.9466 - val_acc: 0.6738\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9863 - acc: 0.6553 - val_loss: 0.9508 - val_acc: 0.6756\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9756 - acc: 0.6587 - val_loss: 0.9988 - val_acc: 0.6632\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9773 - acc: 0.6602 - val_loss: 0.9854 - val_acc: 0.6622\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 0.9772 - acc: 0.6573 - val_loss: 0.9465 - val_acc: 0.6749\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9693 - acc: 0.6612 - val_loss: 0.9343 - val_acc: 0.6792\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9682 - acc: 0.6624 - val_loss: 0.9809 - val_acc: 0.6679\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9570 - acc: 0.6665 - val_loss: 0.9097 - val_acc: 0.6856\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9600 - acc: 0.6637 - val_loss: 0.9383 - val_acc: 0.6820\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.9569 - acc: 0.6675 - val_loss: 0.9302 - val_acc: 0.6821\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9556 - acc: 0.6666 - val_loss: 0.9858 - val_acc: 0.6709\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9484 - acc: 0.6700 - val_loss: 0.9169 - val_acc: 0.6899\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9466 - acc: 0.6695 - val_loss: 0.9215 - val_acc: 0.6812\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9482 - acc: 0.6669 - val_loss: 0.9121 - val_acc: 0.6890\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 0.9431 - acc: 0.6699 - val_loss: 0.9255 - val_acc: 0.6919\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9357 - acc: 0.6723 - val_loss: 0.9356 - val_acc: 0.6811\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9379 - acc: 0.6733 - val_loss: 0.9142 - val_acc: 0.6912\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9370 - acc: 0.6734 - val_loss: 0.9427 - val_acc: 0.6797\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9325 - acc: 0.6762 - val_loss: 0.8978 - val_acc: 0.6945\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9293 - acc: 0.6753 - val_loss: 0.9165 - val_acc: 0.6895\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9355 - acc: 0.6734 - val_loss: 0.8949 - val_acc: 0.6958\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9236 - acc: 0.6781 - val_loss: 0.9123 - val_acc: 0.6908\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9201 - acc: 0.6783 - val_loss: 0.8895 - val_acc: 0.6990\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9190 - acc: 0.6816 - val_loss: 0.9483 - val_acc: 0.6865\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9223 - acc: 0.6792 - val_loss: 0.9155 - val_acc: 0.6912\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9210 - acc: 0.6799 - val_loss: 0.9235 - val_acc: 0.6861\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9145 - acc: 0.6823 - val_loss: 0.8970 - val_acc: 0.6988\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9150 - acc: 0.6817 - val_loss: 0.8771 - val_acc: 0.7034\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 0.9149 - acc: 0.6819 - val_loss: 0.9527 - val_acc: 0.6833\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9115 - acc: 0.6829 - val_loss: 0.9403 - val_acc: 0.6826\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9097 - acc: 0.6839 - val_loss: 0.8945 - val_acc: 0.6936\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9074 - acc: 0.6841 - val_loss: 0.9991 - val_acc: 0.6575\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.9043 - acc: 0.6840 - val_loss: 0.9341 - val_acc: 0.6866\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9080 - acc: 0.6844 - val_loss: 0.9272 - val_acc: 0.6860\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9021 - acc: 0.6847 - val_loss: 0.9160 - val_acc: 0.6897\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9024 - acc: 0.6854 - val_loss: 0.9265 - val_acc: 0.6900\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9015 - acc: 0.6863 - val_loss: 0.8651 - val_acc: 0.7065\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 0.8993 - acc: 0.6859 - val_loss: 0.9120 - val_acc: 0.6934\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.9017 - acc: 0.6865 - val_loss: 0.9124 - val_acc: 0.6882\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8950 - acc: 0.6873 - val_loss: 0.9229 - val_acc: 0.6872\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8982 - acc: 0.6860 - val_loss: 0.9154 - val_acc: 0.6870\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8950 - acc: 0.6887 - val_loss: 0.8849 - val_acc: 0.7056\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8952 - acc: 0.6905 - val_loss: 0.8993 - val_acc: 0.7001\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8918 - acc: 0.6893 - val_loss: 0.8962 - val_acc: 0.6990\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8840 - acc: 0.6916 - val_loss: 0.9164 - val_acc: 0.6927\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8898 - acc: 0.6904 - val_loss: 0.9012 - val_acc: 0.6958\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8869 - acc: 0.6906 - val_loss: 0.9565 - val_acc: 0.6789\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.8848 - acc: 0.6909 - val_loss: 0.8907 - val_acc: 0.7021\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8817 - acc: 0.6935 - val_loss: 0.9165 - val_acc: 0.6959\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8839 - acc: 0.6892 - val_loss: 0.8818 - val_acc: 0.7036\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.8874 - acc: 0.6918 - val_loss: 0.8643 - val_acc: 0.7057\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8824 - acc: 0.6923 - val_loss: 0.9110 - val_acc: 0.6922\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 16s 41ms/step - loss: 0.8858 - acc: 0.6927 - val_loss: 0.8676 - val_acc: 0.7053\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 17s 42ms/step - loss: 0.8843 - acc: 0.6919 - val_loss: 0.9256 - val_acc: 0.6901\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8757 - acc: 0.6944 - val_loss: 0.8762 - val_acc: 0.6993\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8768 - acc: 0.6946 - val_loss: 0.8769 - val_acc: 0.7044\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8818 - acc: 0.6928 - val_loss: 0.8605 - val_acc: 0.7077\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8781 - acc: 0.6942 - val_loss: 0.8418 - val_acc: 0.7137\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8764 - acc: 0.6930 - val_loss: 0.8943 - val_acc: 0.7025\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8737 - acc: 0.6956 - val_loss: 0.9004 - val_acc: 0.7015\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8697 - acc: 0.6965 - val_loss: 0.9198 - val_acc: 0.6943\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 16s 42ms/step - loss: 0.8721 - acc: 0.6960 - val_loss: 0.8785 - val_acc: 0.7006\n",
            "50000/50000 [==============================] - 3s 63us/step\n",
            "Test loss: 0.7640877702903748\n",
            "Test accuracy: 0.7345\n",
            "10000/10000 [==============================] - 1s 63us/step\n",
            "Test loss: 0.8784995346069336\n",
            "Test accuracy: 0.7006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
